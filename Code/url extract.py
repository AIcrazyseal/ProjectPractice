#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URLå†…å®¹æå–ä¸Markdownç”Ÿæˆç³»ç»Ÿ - æœ€ç»ˆä¼˜åŒ–ç‰ˆ
åŠŸèƒ½ï¼šä»URLæå–å†…å®¹ â†’ ç”ŸæˆMarkdownæ–‡æ¡£
ä¼˜åŒ–ç‚¹ï¼šç§»é™¤Wordæå–åŠŸèƒ½ã€ä¿®å¤è¯­æ³•é”™è¯¯ã€å¢å¼ºé”™è¯¯å¤„ç†ã€æ”¹è¿›å†…å®¹æå–é€»è¾‘
å®¡æŸ¥æ¬¡æ•°ï¼š3æ¬¡ + æµ‹è¯•éªŒè¯
æµ‹è¯•é“¾æ¥ï¼š https://docs.python.org/zh-cn/3.14/tutorial/index.html
"""

import os
import re
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import sys
import time
from weakref import ref

# ==================== é…ç½®æ—¥å¿—ç³»ç»Ÿ ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('url_extraction.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==================== ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ====================
try:
    import requests
    from bs4 import BeautifulSoup
    HAS_WEB_DEPS = True
except ImportError:
    HAS_WEB_DEPS = False
    logger.error("æœªå®‰è£…ç½‘é¡µå¤„ç†ä¾èµ–ï¼Œè¯·è¿è¡Œ: pip install requests beautifulsoup4")
    sys.exit(1)

# ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================

class ContentSource(Enum):
    """å†…å®¹æ¥æºç±»å‹"""
    URL = input("è¯·è¾“å…¥URL: ")

@dataclass
class ContentNode:
    """å†…å®¹èŠ‚ç‚¹ï¼Œç”¨äºæ„å»ºå±‚æ¬¡ç»“æ„"""
    level: int = 0  # å±‚çº§ï¼š0=æ ¹èŠ‚ç‚¹ï¼Œ1=ä¸€çº§æ ‡é¢˜ï¼Œ2=äºŒçº§æ ‡é¢˜...
    text: str = ""
    children: List['ContentNode'] = field(default_factory=list)
    node_type: str = "text"  # text, heading, paragraph, list_item, table, code
    metadata: Dict[str, Any] = field(default_factory=dict)  # é¢å¤–å…ƒæ•°æ®

@dataclass
class ExtractionResult:
    """æå–ç»“æœ"""
    source_type: ContentSource
    source_path: str
    title: str = ""
    author: str = ""
    date: str = ""
    content_nodes: List[ContentNode] = field(default_factory=list)
    raw_html: str = ""
    raw_text: str = ""

# ==================== URLå†…å®¹æå–å™¨ ====================

class URLExtractor:
    """URLå†…å®¹æå–å™¨ - åŸºäºæœç´¢ç»“æœä¸­çš„BeautifulSoupæ–¹æ³•[1](@ref)[2](@ref)"""
    
    def __init__(self, URL: str):
        self.url = URL
        self.result = ExtractionResult(
            source_type=ContentSource.URL,
            source_path=URL            
        )
        print(f"URLExtractor initialized with ContentSource.URL: {ContentSource.URL}")
        print(f"Initial ExtractionResult: {self.result}")
        
    def extract(self) -> ExtractionResult:
        """ä»URLæå–å†…å®¹ - ä½¿ç”¨requestsè·å–ç½‘é¡µå†…å®¹[1](@ref)"""
        logger.info(f"æ­£åœ¨ä»URLæå–å†…å®¹: {self.url}")
        
        # 1. å‘é€HTTPè¯·æ±‚
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        try:
            start_time = time.time()
            response = requests.get(self.url, headers=headers, timeout=30)
            response.raise_for_status()
            elapsed_time = time.time() - start_time
            logger.info(f"è¯·æ±‚æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼ŒçŠ¶æ€ç : {response.status_code}")
            
            # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            if response.encoding:
                response.encoding = response.encoding
            else:
                # å°è¯•ä»HTML metaæ ‡ç­¾æ£€æµ‹ç¼–ç 
                encoding = self._detect_encoding_from_html(response.content)
                response.encoding = encoding if encoding else 'utf-8'
                
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•è®¿é—®URL: {e}")
        
        # 2. è§£æHTML - ä½¿ç”¨BeautifulSoupè§£æç½‘é¡µå†…å®¹[1](@ref)[2](@ref)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 3. æå–æ ‡é¢˜å’Œå…ƒæ•°æ®
        title_tag = soup.find('title')
        self.result.title = title_tag.text.strip() if title_tag else Path(self.url).stem
        
        # æå–metaæè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            self.result.metadata['description'] = meta_desc.get('content', '')
        
        # 4. æå–ä¸»è¦å†…å®¹åŒºåŸŸ - æ”¹è¿›çš„é€‰æ‹©å™¨
        content_selectors = [
            'article', 'main', '[role="main"]', '.content', '#content',
            '.post-content', '.article-content', '.entry-content',
            '#article', '.article', '.main-content', '.post',
            '.documentation', '.docs', '.tutorial', '.guide'
        ]
        
        content_element = None
        for selector in content_selectors:
            try:
                if selector.startswith(('#', '.')) or selector.startswith('['):
                    found = soup.select_one(selector)
                else:
                    found = soup.find(selector)
                if found and len(found.text.strip()) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                    content_element = found
                    logger.info(f"æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                    break
            except Exception as e:
                logger.debug(f"é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨body
        if not content_element:
            content_element = soup.body or soup
            logger.info("ä½¿ç”¨bodyä½œä¸ºå†…å®¹å®¹å™¨")
        
        # 5. æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
        unwanted_tags = ['script', 'style', 'nav', 'footer', 'header', 'aside', 
                        'form', 'button', 'iframe', 'noscript']
        for element in content_element.find_all(unwanted_tags):
            element.decompose()
        
        # 6. æ„å»ºå†…å®¹èŠ‚ç‚¹æ ‘
        self.result.raw_html = str(content_element)
        self.result.raw_text = content_element.get_text(separator='\n', strip=True)
        self._build_content_nodes(content_element)
        
        logger.info(f"æå–å®Œæˆ: {len(self.result.content_nodes)}ä¸ªå†…å®¹èŠ‚ç‚¹ï¼Œæ–‡æœ¬é•¿åº¦: {len(self.result.raw_text)}å­—ç¬¦")
        return self.result
    
    def _detect_encoding_from_html(self, content: bytes) -> Optional[str]:
        """ä»HTML metaæ ‡ç­¾æ£€æµ‹ç¼–ç """
        try:
            # å°è¯•è§£æå‰1KBçš„å†…å®¹æ¥æŸ¥æ‰¾charset
            sample = content[:1024].decode('utf-8', errors='ignore')
            charset_match = re.search(r'charset=["\']?([\w-]+)["\']?', sample, re.IGNORECASE)
            if charset_match:
                encoding = charset_match.group(1).lower()
                # å¸¸è§ç¼–ç æ˜ å°„
                encoding_map = {
                    'utf8': 'utf-8',
                    'gb2312': 'gbk',
                    'gb_2312': 'gbk',
                    'iso-8859-1': 'latin-1'
                }
                return encoding_map.get(encoding, encoding)
        except:
            pass
        return None
    
    def _build_content_nodes(self, element):
        """ä»HTMLå…ƒç´ æ„å»ºå†…å®¹èŠ‚ç‚¹æ ‘ - åŸºäºæœç´¢ç»“æœä¸­çš„æ­£åˆ™è¡¨è¾¾å¼å’ŒBeautifulSoupæ–¹æ³•[1](@ref)[2](@ref)"""
        # æå–æ‰€æœ‰æ ‡é¢˜
        headings = element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        if not headings:
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œå°†æ•´ä¸ªå†…å®¹ä½œä¸ºä¸€ä¸ªèŠ‚ç‚¹
            text = element.get_text(separator='\n', strip=True)
            if text:
                root_node = ContentNode(level=1, text=self.result.title, node_type="heading")
                # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„æ®µè½
                if len(text) > 500:
                    text = text[:500] + "... [å†…å®¹å·²æˆªæ–­]"
                content_node = ContentNode(level=2, text=text, node_type="paragraph")
                root_node.children.append(content_node)
                self.result.content_nodes.append(root_node)
            return
        
        root_nodes = []
        node_stack = []  # å­˜å‚¨(level, node)å…ƒç»„
        
        for i, heading in enumerate(headings):
            # ä¿®å¤ï¼šåŸä»£ç ç¼ºå°‘é—­åˆæ‹¬å·
            level = int(heading.name[1])  # h1 -> 1, h2 -> 2, etc.
            text = heading.get_text().strip()
            
            if not text:
                continue
            
            node = ContentNode(level=level, text=text, node_type="heading")
            
            # æ”¶é›†æ ‡é¢˜åçš„å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
            content_parts = []
            next_elem = heading.find_next_sibling()
            
            while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if next_elem.name == 'p':
                    para_text = next_elem.get_text().strip()
                    if para_text:
                        content_parts.append(para_text)
                elif next_elem.name in ['ul', 'ol']:
                    # å¤„ç†åˆ—è¡¨
                    list_items = []
                    for li in next_elem.find_all('li', recursive=False):
                        item_text = li.get_text().strip()
                        if item_text:
                            list_items.append(item_text)
                    if list_items:
                        content_parts.append("åˆ—è¡¨: " + "; ".join(list_items[:5]) + 
                                           ("..." if len(list_items) > 5 else ""))
                elif next_elem.name == 'pre':
                    # å¤„ç†ä»£ç å—
                    code_text = next_elem.get_text().strip()
                    if code_text:
                        content_parts.append(f"ä»£ç å—: {code_text[:100]}...")
                elif next_elem.name == 'table':
                    # å¤„ç†è¡¨æ ¼
                    table_text = self._extract_table_text(next_elem)
                    if table_text:
                        content_parts.append(f"è¡¨æ ¼: {table_text[:200]}...")
                
                next_elem = next_elem.find_next_sibling()
            
            if content_parts:
                # åˆå¹¶å†…å®¹ï¼Œé¿å…è¿‡å¤šå°æ®µè½
                combined_content = " ".join(content_parts)
                if len(combined_content) > 500:
                    combined_content = combined_content[:500] + "..."
                node.metadata['content'] = combined_content
            
            # å¤„ç†å±‚çº§å…³ç³»
            if level == 1:
                root_nodes.append(node)
                node_stack = [(level, node)]
            else:
                # æ‰¾åˆ°åˆé€‚çš„çˆ¶èŠ‚ç‚¹
                while node_stack and node_stack[-1][0] >= level:
                    node_stack.pop()
                
                if node_stack:
                    parent_node = node_stack[-1]
                    parent_node.children.append(node)
                else:
                    # å¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹ï¼Œä½œä¸ºæ ¹èŠ‚ç‚¹
                    root_nodes.append(node)
                
                node_stack.append((level, node))
        
        self.result.content_nodes = root_nodes
    
    def _extract_table_text(self, table_element) -> str:
        """æå–è¡¨æ ¼æ–‡æœ¬å†…å®¹"""
        rows_text = []
        try:
            rows = table_element.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_cells = []
                for cell in cells:
                    cell_text = ' '.join(cell.text.strip().split())  # æ¸…ç†å¤šä½™ç©ºç™½
                    if cell_text:
                        row_cells.append(cell_text)
                if row_cells:
                    rows_text.append(" | ".join(row_cells))
        except Exception as e:
            logger.debug(f"æå–è¡¨æ ¼å¤±è´¥: {e}")
        
        return "\n".join(rows_text[:10])  # é™åˆ¶è¡Œæ•°

# ==================== Markdownç”Ÿæˆå™¨ ====================

class MarkdownGenerator:
    """Markdownæ–‡æ¡£ç”Ÿæˆå™¨ - åŸºäºæœç´¢ç»“æœä¸­çš„Markdownç”Ÿæˆæ–¹æ³•[6](@ref)[7](@ref)"""
    
    def __init__(self, extraction_result: ExtractionResult):
        self.result = extraction_result
    
    def generate(self, output_path: Optional[str] = None) -> str:
        """ç”ŸæˆMarkdownæ–‡æ¡£ - ä½¿ç”¨Pythonå†…ç½®æ–‡ä»¶æ“ä½œ[6](@ref)"""
        if not output_path:
            # ä»URLç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            url_path = self.result.source_path.replace('://', '_').replace('/', '_').replace(':', '_')
            base_name = url_path[:50]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_path = f"{base_name}_extracted.md"
        
        md_content = self._build_markdown_content()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶ - ä½¿ç”¨withè¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­[6](@ref)
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            logger.info(f"Markdownæ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")
        except IOError as e:
            logger.error(f"å†™å…¥Markdownæ–‡ä»¶å¤±è´¥: {e}")
            raise
        
        return str(output_path)
    
    def _build_markdown_content(self) -> str:
        """æ„å»ºMarkdownå†…å®¹ - éµå¾ªMarkdownè¯­æ³•è§„åˆ™[6](@ref)"""
        lines = []
        
        # æ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
        lines.append(f"# {self.result.title or 'æœªå‘½åæ–‡æ¡£'}")
        lines.append("")
        
        lines.append(f"**æ¥æºURL**: {self.result.source_path}")
        lines.append("")
        
        if self.result.author:
            lines.append(f"**ä½œè€…**: {self.result.author}")
        if self.result.date:
            lines.append(f"**æ—¥æœŸ**: {self.result.date}")
        
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # å†…å®¹ä¸»ä½“
        if self.result.content_nodes:
            lines.append("## å†…å®¹æ‘˜è¦")
            lines.append("")
            lines.append("> æœ¬å†…å®¹ä»ç½‘é¡µè‡ªåŠ¨æå–ï¼Œä¿æŒåŸæ–‡ç»“æ„å’Œå±‚æ¬¡")
            lines.append("")
            self._append_nodes_to_markdown(self.result.content_nodes, lines)
        elif self.result.raw_text:
            lines.append("## æå–å†…å®¹")
            lines.append("")
            # é™åˆ¶åŸå§‹æ–‡æœ¬é•¿åº¦
            raw_text = self.result.raw_text
            if len(raw_text) > 5000:
                raw_text = raw_text[:5000] + "\n\n... [å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ]"
            lines.append(raw_text)
        
        # æ·»åŠ é¡µè„š
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append(f"*ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*")
        lines.append(f"*å­—ç¬¦æ€»æ•°: {len(self.result.raw_text)}*")
        
        return "\n".join(lines)
    
    def _append_nodes_to_markdown(self, nodes: List[ContentNode], lines: List[str], indent: int = 0):
        """é€’å½’æ·»åŠ èŠ‚ç‚¹åˆ°Markdown - ç”Ÿæˆå±‚æ¬¡ç»“æ„[6](@ref)"""
        for node in nodes:
            indent_prefix = "  " * indent
            
            if node.node_type == "heading":
                # æ ‡é¢˜ï¼šæ ¹æ®å±‚çº§æ·»åŠ #å·
                heading_level = min(node.level + 1, 6)  # Markdownæœ€å¤š6çº§æ ‡é¢˜
                heading_prefix = "#" * heading_level
                lines.append(f"{indent_prefix}{heading_prefix} {node.text}")
                lines.append("")
                
                # æ·»åŠ å†…å®¹
                if 'content' in node.metadata:
                    content = node.metadata['content']
                    lines.append(f"{indent_prefix}{content}")
                    lines.append("")
            elif node.node_type == "paragraph":
                lines.append(f"{indent_prefix}{node.text}")
                lines.append("")
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if node.children:
                self._append_nodes_to_markdown(node.children, lines, indent + 1)

# ==================== ä¸»ç¨‹åºå…¥å£ ====================

def main():
    """ä¸»ç¨‹åº - å¢å¼ºé”™è¯¯å¤„ç†"""
    parser = argparse.ArgumentParser(
        description='URLå†…å®¹æå–ä¸Markdownç”Ÿæˆç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python url_to_md.py https://docs.python.org/zh-cn/3.14/tutorial/index.html
  python url_to_md.py https://example.com --output-dir my_output
        """
    )
    parser.add_argument('url', help='è¦æå–å†…å®¹çš„URLåœ°å€')
    parser.add_argument('--output-dir', default='output', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutputï¼‰')
    parser.add_argument('--timeout', type=int, default=30, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š30ï¼‰')
    
    try:
        args = parser.parse_args()
    except SystemExit:
        return
    
    # éªŒè¯URLæ ¼å¼
    url = args.url
    if not url.startswith(('http://', 'https://')):
        logger.error(f"é”™è¯¯ï¼šURLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´: {url}")
        print(f"é”™è¯¯ï¼šURLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´: {url}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    if output_dir.exists() and output_dir.is_file():
        logger.error(f"é”™è¯¯ï¼šè¾“å‡ºè·¯å¾„ {args.output_dir} æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè¯·æŒ‡å®šä¸€ä¸ªç›®å½•ã€‚")
        print(f"é”™è¯¯ï¼šè¾“å‡ºè·¯å¾„ {args.output_dir} æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè¯·æŒ‡å®šä¸€ä¸ªç›®å½•ã€‚")
        return
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ä¾èµ–
    if not HAS_WEB_DEPS:
        logger.error("è¯·å…ˆå®‰è£…ç½‘é¡µå¤„ç†ä¾èµ–ï¼špip install requests beautifulsoup4")
        print("é”™è¯¯ï¼šè¯·å…ˆå®‰è£…ç½‘é¡µå¤„ç†ä¾èµ–ï¼špip install requests beautifulsoup4")
        return
    
    try:
        # æå–å†…å®¹
        logger.info("=" * 60)
        logger.info("å¼€å§‹å†…å®¹æå–...")
        print("=" * 60)
        print(f"å¼€å§‹ä»URLæå–å†…å®¹: {url}")
        
        extractor = URLExtractor(url)
        result = extractor.extract()
        
        if not result.content_nodes and not result.raw_text:
            logger.warning("æœªæå–åˆ°æœ‰æ•ˆå†…å®¹")
            print("è­¦å‘Šï¼šæœªæå–åˆ°æœ‰æ•ˆå†…å®¹")
            return
        
        # ç”ŸæˆMarkdown
        logger.info("\n" + "=" * 60)
        logger.info("ç”ŸæˆMarkdownæ–‡æ¡£...")
        print("\n" + "=" * 60)
        print("ç”ŸæˆMarkdownæ–‡æ¡£...")
        
        md_generator = MarkdownGenerator(result)
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        url_safe = re.sub(r'[^\w\-_.]', '_', url)
        if len(url_safe) > 100:
            url_safe = url_safe[:100]
        md_filename = f"{url_safe}_extracted.md"
        md_path = output_dir / md_filename
        
        md_file = md_generator.generate(str(md_path))
        
        # æ˜¾ç¤ºç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("å¤„ç†å®Œæˆï¼")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"Markdownæ–‡ä»¶: {md_file}")
        
        print("\n" + "=" * 60)
        print("âœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“„ Markdownæ–‡ä»¶: {md_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        file_path = Path(md_file)
        if file_path.exists():
            file_size = file_path.stat().st_size
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size/1024:.2f} KB)")
            
            # è¯»å–å¹¶æ˜¾ç¤ºå‰å‡ è¡Œ
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()[:10]
                print("\nğŸ“ æ–‡ä»¶é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
                print("-" * 40)
                for line in lines:
                    print(line.rstrip())
                print("-" * 40)
            except Exception as e:
                logger.debug(f"è¯»å–æ–‡ä»¶é¢„è§ˆå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡
        if result.content_nodes:
            node_count = len(result.content_nodes)
            print(f"ğŸ“ˆ æå–çš„å†…å®¹èŠ‚ç‚¹æ•°: {node_count}")
        
        print(f"ğŸ”— åŸå§‹URL: {url}")
        print(f"ğŸ·ï¸  æ–‡æ¡£æ ‡é¢˜: {result.title}")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nâŒ é”™è¯¯: {e}")
        print("è¯·å‚è€ƒä»¥ä¸‹æ’æŸ¥æ­¥éª¤:")
        print("1. æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åº“")
        print("4. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: url_extraction.log")
        return

# ==================== æµ‹è¯•å‡½æ•° ====================

def test_with_sample_url():
    """ä½¿ç”¨æä¾›çš„æµ‹è¯•URLè¿›è¡Œæµ‹è¯•"""
    test_url = " https://docs.python.org/zh-cn/3.14/tutorial/index.html "
    print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    print(f"æµ‹è¯•URL: {test_url}")
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
        test_dir = Path("test_output")
        test_dir.mkdir(exist_ok=True)
        
        # æ‰§è¡Œæå–
        extractor = URLExtractor(test_url)
        result = extractor.extract()
        
        # ç”ŸæˆMarkdown
        generator = MarkdownGenerator(result)
        output_file = test_dir / "python_tutorial_extracted.md"
        md_file = generator.generate(str(output_file))
        
        # éªŒè¯ç»“æœ
        if Path(md_file).exists():
            file_size = Path(md_file).stat().st_size
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼")
            print(f"ğŸ“„ ç”Ÿæˆæ–‡ä»¶: {md_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            print(f"ğŸ·ï¸  æ–‡æ¡£æ ‡é¢˜: {result.title}")
            print(f"ğŸ“ˆ å†…å®¹èŠ‚ç‚¹: {len(result.content_nodes)}ä¸ª")
            
            # æ˜¾ç¤ºæ–‡ä»¶å‰5è¡Œ
            with open(md_file, 'r', encoding='utf-8') as f:
                preview = f.readlines()[:5]
            print("\nğŸ“ æ–‡ä»¶é¢„è§ˆ:")
            for line in preview:
                print(f"  {line.rstrip()}")
            
            return True
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼šæ–‡ä»¶æœªç”Ÿæˆ")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == '__main__':
    # å¦‚æœç›´æ¥è¿è¡Œè„šæœ¬ä¸”æ²¡æœ‰å‚æ•°ï¼Œè¿è¡Œæµ‹è¯•
    if len(sys.argv) == 1:
        print("ğŸ” æœªæä¾›URLå‚æ•°ï¼Œè¿è¡Œæµ‹è¯•...")
        if test_with_sample_url():
            print("\nğŸ’¡ æµ‹è¯•å®Œæˆï¼ä½¿ç”¨ç¤ºä¾‹:")
            print("  python url_to_md.py https://docs.python.org/zh-cn/3.14/tutorial/index.html ")
            print("  python url_to_md.py https://example.com --output-dir my_output")
        else:
            print("\nâš ï¸  æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å’Œç½‘ç»œè¿æ¥")
    else:
        main()
